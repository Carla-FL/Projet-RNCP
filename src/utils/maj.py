import sys
import os
import pathlib
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import json
import logging
from datetime import datetime, timedelta
from dotenv import load_dotenv
from src.Pipeline1.etl import main_etl
import pandas as pd

# Charger les variables d'environnement
load_dotenv()

# Configuration du logging pour GitHub Actions
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('logs/sync.log', 'a', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class YouTubeSynchronizer:
    """Gestionnaire de synchronisation YouTube pour GitHub Actions"""
    
    def __init__(self):
        self.force_update = os.getenv('FORCE_UPDATE', 'false').lower() == 'true'
        self.target_channel = os.getenv('TARGET_CHANNEL', '')
        self.summary = {
            'start_time': datetime.now().isoformat(),
            'records_processed': 0,
            'videos_updated': 0,
            'errors': [],
            'channels_processed': []
        }
        
    def detect_environment(self):
        """D√©tecte si on est dans GitHub Actions ou en local"""
        if os.getenv('GITHUB_ACTIONS'):
            logger.info("Environnement d√©tect√©: GitHub Actions")
            return 'github_actions'
        elif os.getenv('STREAMLIT_SERVER_HEADLESS'):
            logger.info("Environnement d√©tect√©: Streamlit Cloud")
            return 'streamlit_cloud'
        else:
            logger.info("Environnement d√©tect√©: Local")
            return 'local'
    
    def setup_imports(self):
        """Configure les imports selon l'environnement"""
        try:
            # Imports sp√©cifiques √† ton projet
            from load import Load
            from transformation import main_transformation

            
            self.Load = Load
            self.main_transformation = main_transformation
            logger.info("Modules import√©s avec succ√®s")
            return True
            
        except ImportError as e:
            logger.error(f"Erreur d'import: {e}")
            self.summary['errors'].append(f"Import error: {str(e)}")
            return False
    
    def get_channels_to_process(self):
        """D√©termine quels channels traiter"""
        if self.target_channel:
            logger.info(f"Traitement du channel sp√©cifique: {self.target_channel}")
            return [self.target_channel]
        
        # Liste de tes channels configur√©s

        channels = []
        
        # Option 1: Depuis variables d'environnement
        channels_env = os.getenv('YOUTUBE_CHANNELS', '')
        if channels_env:
            channels = channels_env.split(',')
        
        # Option 2: Liste hardcod√©e (√† adapter)
        if not channels:
            channels = [
                'UCDkl5M0WVaddTWE4rr2cSeA',
            ]
        
        logger.info(f"Channels √† traiter: {len(channels)} - {channels}")
        return channels
    
    def get_videos_for_channel(self, channel_id):
        """R√©cup√®re les vid√©os √† traiter pour un channel"""
        try:
            # Utiliser ton syst√®me existant pour r√©cup√©rer les vid√©os
            # √Ä adapter selon ton code d'extraction
            
            # Exemple simplifi√© - remplace par ton code r√©el
            loader = self.Load()
            
            client = loader.data_base_connexion()

            # etape 2 : r√©cup√©rer les urls des vid√©os de la base de donn√©es
            list_urls = []
            # id_lists = []  # pour stocker les derniers ids de commentaires

            for db_name in client.list_database_names():
                # logger.info(f"traitement base {db_name}")
                if db_name in ("admin", "config", "local", "test"):  # filtrer les bases internes MongoDB
                    # logger.info(f"bases trouv√©es {db_name}")
                    continue
                db = client[db_name]
                # Lister toutes les collections de cette base
                for collection in db.list_collection_names():
                    # logger.info(f"traitement de la collection {collection}")
                    db_collection = db[collection]
                    # R√©cup√©rer l'URL de la vid√©o √† partir de la collection
                    video_url = db_collection.find_one({}, {"url": 1, "_id": 0})
                    logger.info(f"taille de video_url {len(video_url)}")
                    if len(video_url)>0:
                        list_urls.append(video_url['url'])
                    else:
                        pass # ajouter un truc pour ajouter les info s'il n'y a pas d'url ajouter un param√®tre a main_etl
            loader.data_base_deconnexion(client)
            
            logger.info(f"Channel {channel_id}: {len(list_urls)} vid√©os √† traiter")
            return list_urls
            
        except Exception as e:
            logger.error(f"Erreur r√©cup√©ration vid√©os pour {channel_id}: {e}")
            self.summary['errors'].append(f"Channel {channel_id}: {str(e)}")
            return []
    
    def process_video(self, channel_id, video_url):
        """Traite une vid√©o sp√©cifique"""
        try:
            logger.info(f"Traitement de : {video_url} vid√©os. (Channel: {channel_id})")
            
            # √âtape 1: Extraction des donn√©es
            # √Ä remplacer par ton code d'extraction r√©el
            # df_extracted = self.main_extraction(video_id, channel_id)
            
            # Pour l'instant, simulation
            df_extracted = video_url
            
            if df_extracted is None or df_extracted.empty:
                logger.warning(f"Aucune donn√©e extraite pour {video_url.split("v=")[-1][:11]}")
                return False
            
            # √âtape 2: Transformation des donn√©es
            df_transformed = main_etl(video_url, maj=True)
            
            if df_transformed.empty:
                logger.warning(f"Aucune donn√©e apr√®s transformation pour {video_url.split("v=")[-1][:11]}")
                return False
            
            # # √âtape 3: Chargement en base
            # loader = self.Load(maj=True)  # Mode mise √† jour
            # loader.load(df_transformed, video_id, channel_id)
            
            # Mise √† jour des statistiques
            self.summary['records_processed'] += len(df_transformed)
            self.summary['videos_updated'] += 1
            
            logger.info(f"Vid√©o {video_url} trait√©e avec succ√®s: {len(df_transformed)} commentaires")
            return True
            
        except Exception as e:
            logger.error(f"Erreur traitement vid√©o {video_url}: {e}")
            self.summary['errors'].append(f"Video {video_url}: {str(e)}")
            return False
    
    def process_channel(self, channel_id):
        """Traite un channel complet"""
        try:
            logger.info(f"D√©but traitement channel: {channel_id}")
            
            # R√©cup√©rer les vid√©os √† traiter
            videos = self.get_videos_for_channel(channel_id)
            
            if not videos:
                logger.info(f"Aucune vid√©o √† traiter pour le channel {channel_id}")
                return True
            
            # Traiter chaque vid√©o
            success_count = 0
            for video_url in videos:
                if self.process_video(channel_id, video_url):
                    success_count += 1
                
                # Pause pour √©viter les limites de rate
                import time
                time.sleep(1)
            
            self.summary['channels_processed'].append({
                'channel_id': channel_id,
                'videos_processed': success_count,
                'total_videos': len(videos)
            })
            
            logger.info(f"Channel {channel_id} termin√©: {success_count}/{len(videos)} vid√©os trait√©es")
            return True
            
        except Exception as e:
            logger.error(f"Erreur traitement channel {channel_id}: {e}")
            self.summary['errors'].append(f"Channel {channel_id}: {str(e)}")
            return False
    
    def run_synchronization(self):
        """Ex√©cute la synchronisation compl√®te"""
        logger.info("üöÄ D√©but de la synchronisation YouTube")
        
        # V√©rifier l'environnement
        env = self.detect_environment()
        
        # Configurer les imports
        if not self.setup_imports():
            logger.error("Impossible de configurer les imports")
            return False
        
        # R√©cup√©rer les channels √† traiter
        channels = self.get_channels_to_process()
        
        if not channels:
            logger.error("Aucun channel √† traiter")
            return False
        
        # Traiter chaque channel
        success = True
        for channel_id in channels:
            if not self.process_channel(channel_id):
                success = False
        
        # Finaliser
        self.summary['end_time'] = datetime.now().isoformat()
        self.summary['duration_minutes'] = (
            datetime.fromisoformat(self.summary['end_time']) - 
            datetime.fromisoformat(self.summary['start_time'])
        ).total_seconds() / 60
        
        self.save_summary()
        
        if success:
            logger.info("‚úÖ Synchronisation termin√©e avec succ√®s")
        else:
            logger.warning("‚ö†Ô∏è Synchronisation termin√©e avec des erreurs")
        
        return success
    
    def save_summary(self):
        """Sauvegarde le r√©sum√© de l'ex√©cution"""
        try:
            os.makedirs('logs', exist_ok=True)
            
            # Sauvegarder le summary en JSON
            with open('logs/summary.json', 'w') as f:
                json.dump(self.summary, f, indent=2, ensure_ascii=False)
            
            # Log du r√©sum√©
            logger.info("üìä R√©sum√© de l'ex√©cution:")
            logger.info(f"  - Dur√©e: {self.summary.get('duration_minutes', 0):.1f} minutes")
            logger.info(f"  - Records trait√©s: {self.summary['records_processed']}")
            logger.info(f"  - Vid√©os mises √† jour: {self.summary['videos_updated']}")
            logger.info(f"  - Erreurs: {len(self.summary['errors'])}")
            
            if self.summary['errors']:
                logger.warning("Erreurs rencontr√©es:")
                for error in self.summary['errors'][:5]:  # Limiter √† 5 pour les logs
                    logger.warning(f"  - {error}")
            
        except Exception as e:
            logger.error(f"Erreur sauvegarde summary: {e}")

def main():
    """Point d'entr√©e principal"""
    try:
        # Cr√©er le dossier logs si n√©cessaire
        os.makedirs('logs', exist_ok=True)
        
        # Initialiser et lancer la synchronisation
        synchronizer = YouTubeSynchronizer()
        success = synchronizer.run_synchronization()
        
        # Code de sortie pour GitHub Actions
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        logger.info("Synchronisation interrompue par l'utilisateur")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"Erreur fatale: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()