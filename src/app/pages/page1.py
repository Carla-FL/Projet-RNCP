"""_________________________________ Dashboard Page 1 ___________________________________"""
import pathlib
rootdir = pathlib.Path(__file__).parent.parent.parent.parent.resolve()
import sys
sys.path.append(str(rootdir))
import streamlit as st
import json
import os
from src.Pipeline1.etl import main_etl
import matplotlib.pyplot as plt
import pandas as pd
from src.utils.load import Load
from src.utils.extraction import Extraction
from wordcloud import WordCloud



def get_existing_db(client, videoid):
    # for db_name in client.list_database_names():
    #     if db_name in ("admin", "config", "local"):  # filtrer les bases internes MongoDB
    #         continue
    #     db = client[db_name]
    #     # Lister toutes les collections de cette base
    #     collections = db.list_collection_names()
    #     if videoid in collections:
    #         return db.name
    #     #else:
    #         #raise Exception("La vid√©o n'existe pas dans la base de donn√©es. Veuillez lancer l'extraction des donn√©es.")
    try:
        # Mode cloud : une seule base
        from streamlit_config import get_database_connections
        db_config = get_database_connections()
        db_name = db_config["mongodb"]["database"]
        
        # V√©rifier si la vid√©o existe dans cette base
        db = client[db_name]
        collections = db.list_collection_names()
        
        # Chercher la collection qui contient cette vid√©o
        for collection_name in collections:
            if videoid in collection_name:
                return db_name
                
    except ImportError:
        # Mode local : bases multiples
        for db_name in client.list_database_names():
            if db_name in ("admin", "config", "local"):
                continue
            db = client[db_name]
            if videoid in db.list_collection_names():
                return db_name
    
    return None


def get_kpi(client, db_name, videoid):
    # viedo_title = db[videoid].find_one(sort=[('publishedAt', -1)])['titre']
    # synchronisation_date = db[videoid].find_one(sort=[('extractedAt', -1)])['extractedAt']
    # total_comments = db[videoid].count_documents({})
    # most_liked_comment = db[videoid].find_one(sort=[("likeCount", -1)])["comment"]
    # most_liked_comment_publish_date = db[videoid].find_one(sort=[("likeCount", -1)])["publishedAt"]
    # like_count = db[videoid].find_one(sort=[("likeCount", -1)])["likeCount"]
    
    # return viedo_title, synchronisation_date, total_comments, most_liked_comment, most_liked_comment_publish_date, like_count
    try:
        db = client[db_name]
        
        # D√©terminer le nom de la collection
        try:
            from streamlit_config import get_database_connections
            # Mode cloud : collection pr√©fix√©e
            collections = db.list_collection_names()
            collection_name = None
            for coll in collections:
                if videoid in coll:
                    collection_name = coll
                    break
        except ImportError:
            # Mode local : collection = videoid
            collection_name = videoid
        
        if not collection_name or collection_name not in db.list_collection_names():
            raise ValueError(f"Collection pour vid√©o {videoid} non trouv√©e")
        
        collection = db[collection_name]
        
        # V√©rifier qu'il y a des documents
        if collection.count_documents({}) == 0:
            raise ValueError("Collection vide")
        
        # R√©cup√©ration s√©curis√©e des donn√©es
        latest_doc = collection.find_one(sort=[('publishedAt', -1)])
        if not latest_doc:
            raise ValueError("Aucun document trouv√©")
        
        # Extraction s√©curis√©e des champs
        video_title = latest_doc.get('titre', 'Titre non disponible')
        sync_date = latest_doc.get('extractedAt', 'Date inconnue')
        
        # Statistiques
        total_comments = collection.count_documents({})
        
        # Commentaire le plus aim√©
        most_liked_doc = collection.find_one(
            {"likeCount": {"$exists": True}},
            sort=[("likeCount", -1)]
        )
        
        if most_liked_doc:
            most_liked_comment = most_liked_doc.get('comment', 'Commentaire non disponible')
            most_liked_date = most_liked_doc.get('publishedAt', 'Date inconnue')
            like_count = most_liked_doc.get('likeCount', 0)
        else:
            most_liked_comment = "Aucun commentaire avec likes"
            most_liked_date = "N/A"
            like_count = 0
        
        return video_title, sync_date, total_comments, most_liked_comment, most_liked_date, like_count
        
    except Exception as e:
        st.error(f"Erreur dans get_kpi: {str(e)}")
        return "Erreur", "Erreur", 0, "Erreur", "Erreur", 0

def initialize_session_state():
    """Initialise les variables de session si elles n'existent pas"""
    if 'analysis_done' not in st.session_state:
        st.session_state.analysis_done = False
    if 'video_data' not in st.session_state:
        st.session_state.video_data = {}
    if 'url_input' not in st.session_state:
        st.session_state.url_input = ""


def make_wordcloud(text, where=None):
    """
    G√©n√®re un nuage de mots √† partir d'un texte donn√©.
    
    Args:
        text (str): Le texte √† analyser pour le nuage de mots.
    
    Returns:
        None
    """
    wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=False).generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    # plt.show()
    if where:
        where.pyplot(plt)
    else:
        st.pyplot(plt)


def display_analysis_results():
    """Affiche les r√©sultats de l'analyse stock√©s dans session_state"""
    if not st.session_state.analysis_done or not st.session_state.video_data:
        return False
    
    # R√©cup√©rer les donn√©es stock√©es
    data = st.session_state.video_data
    
    # Afficher le titre de la vid√©o analys√©e
    st.header(f"Analyse de la vid√©o :")
    st.subheader(f"{data['video_title']}")
    
    # Afficher la vid√©o
    st.video(data['url'])
    
    # Afficher la date de mise √† jour
    st.caption(f"Date de derni√®re mise √† jour : {data['sync_date']}")
    st.header("Analyse de premier niveau")
    
    # Cr√©er les colonnes
    left, right = st.columns([1,1], border=False)
    
    # Afficher les m√©triques
    left.metric(label="Nombre total de commentaires", value=data['total_comments'], border=True)
    
    # Container pour le commentaire le plus aim√©
    left_container = left.container(border=True)
    left_container.header("Commentaire le plus aim√©")
    left_container.subheader(data['most_liked_comment'])
    left_container.caption(f"Publi√© le : {data['most_liked_date']}")
    left_container.caption(f"Nombre de likes : {data['like_count']}")
    
    # G√©n√©rer le wordcloud
    if data['wordcloud_text']:
        make_wordcloud(data['wordcloud_text'], right)
    
    # Graphique des commentaires par jour
    if data['comments_per_day']:
        import plotly.express as px
        fig = px.bar(x=data['comments_per_day']['dates'], 
                    y=data['comments_per_day']['counts'],
                    title='Nombre de commentaires par jour',
                    labels={'x': 'Date', 'y': 'Nombre de commentaires'})
        st.plotly_chart(fig, use_container_width=True)
    
    return True


def perform_analysis(url):
    """Effectue l'analyse et stocke les r√©sultats dans session_state"""
    # try:
    #     extract = Extraction(video_url=url)
    #     videoid = extract.url2id()
    #     st.session_state['videoid'] = videoid
    # except Exception as e:
    #     st.error(f"Erreur veuillez entrer une URL YouTube valide. : {e}")
    #     return False
    
    # client = Load().data_base_connexion()
    # st.session_state['client_mdb'] = client
    
    # try:
    #     db_name = get_existing_db(client, videoid)
    #     st.session_state['data_base_name'] = str(db_name)
    # except Exception as e:
    #     st.error(f"Erreur lors de get_existing_db : {e}")
    #     return False
    
    # data_exists = False
    # try:
    #     db = client[db_name]
    #     if videoid in db.list_collection_names():
    #         st.success("La vid√©o existe d√©j√† dans la base de donn√©es")
    #         data_exists = True
    # except Exception as e:
    #     data_exists = False
    #     st.error(f"Les donn√©es n'existent pas encore, on lance l'extraction des donn√©es ...")
        
    #     if not data_exists:
    #         with st.spinner("Wait for it...", show_time=True):
    #             chanel_id = main_etl(url, with_channel_id=True)
    #             st.success("Done!")
            
    #         try:
    #             # st.write(f"Base de donn√©es utilis√©e : {chanel_id}")
    #             db = client[chanel_id]
    #         except Exception as e:
    #             st.error(f"Erreur lors de l'extraction du channel_id : {e}")
    #             return False
    
    # if db is not None:
    #     # R√©cup√©rer les KPI
    #     video_title, sync_date, total_comments, most_liked_comment, most_liked_date, like_count = get_kpi(db, videoid)
        
    #     # Pr√©parer les donn√©es du DataFrame
    #     df = pd.DataFrame(list(db[videoid].find()))
    #     text = df['comment_clean_lem'].dropna()
    #     wordcloud_text = " ".join(text)
        
    #     # Donn√©es pour le graphique
    #     df_chart = pd.DataFrame(list(db[videoid].find(
    #         {},
    #         {"publishedAt": 1, "comment": 1, "_id": 0}
    #     )))
    #     df_chart['date'] = pd.to_datetime(df_chart['publishedAt']).dt.date
    #     comments_per_day = df_chart['date'].value_counts().sort_index()
        
    #     # Stocker toutes les donn√©es dans session_state
    #     st.session_state.video_data = {
    #         'url': url,
    #         'video_title': video_title,
    #         'sync_date': sync_date,
    #         'total_comments': total_comments,
    #         'most_liked_comment': most_liked_comment,
    #         'most_liked_date': most_liked_date,
    #         'like_count': like_count,
    #         'wordcloud_text': wordcloud_text,
    #         'comments_per_day': {
    #             'dates': comments_per_day.index.tolist(),
    #             'counts': comments_per_day.values.tolist()
    #         }
    #     }
        
    #     st.session_state.analysis_done = True
    #     Load().data_base_deconnexion(client)
    #     return True
    
    # return False
    try:
        extract = Extraction(video_url=url)
        videoid = extract.url2id()
        st.session_state['videoid'] = videoid
    except Exception as e:
        st.error(f"URL YouTube invalide: {e}")
        return False
    
    client = Load().data_base_connexion()
    st.session_state['client_mdb'] = client
    
    # Chercher si la vid√©o existe d√©j√†
    db_name = get_existing_db(client, videoid)
    data_exists = db_name is not None
    
    if not data_exists:
        st.info("Vid√©o non trouv√©e, lancement de l'extraction...")
        with st.spinner("Extraction en cours..."):
            try:
                channel_id = main_etl(url, with_channel_id=True)
                if channel_id:
                    db_name = channel_id
                    st.success("Extraction termin√©e!")
                else:
                    st.error("√âchec de l'extraction")
                    return False
            except Exception as e:
                st.error(f"Erreur lors de l'extraction: {e}")
                return False
    else:
        st.success("Vid√©o trouv√©e dans la base de donn√©es")
    
    st.session_state['data_base_name'] = db_name
    
    # R√©cup√©rer les KPI
    try:
        video_title, sync_date, total_comments, most_liked_comment, most_liked_date, like_count = get_kpi(client, db_name, videoid)
        
        # Pr√©parer le wordcloud
        try:
            # Mode cloud/local
            try:
                from streamlit_config import get_database_connections
                collections = client[db_name].list_collection_names()
                collection_name = None
                for coll in collections:
                    if videoid in coll:
                        collection_name = coll
                        break
            except ImportError:
                collection_name = videoid
            
            if collection_name:
                df = pd.DataFrame(list(client[db_name][collection_name].find()))
                if 'comment_clean_lem' in df.columns:
                    text = df['comment_clean_lem'].dropna()
                    wordcloud_text = " ".join(text)
                else:
                    wordcloud_text = "Donn√©es de text nettoy√© non disponibles"
                
                # Donn√©es pour graphique
                if 'publishedAt' in df.columns:
                    df['date'] = pd.to_datetime(df['publishedAt']).dt.date
                    comments_per_day = df['date'].value_counts().sort_index()
                    chart_data = {
                        'dates': comments_per_day.index.tolist(),
                        'counts': comments_per_day.values.tolist()
                    }
                else:
                    chart_data = {'dates': [], 'counts': []}
            else:
                wordcloud_text = "Erreur de collection"
                chart_data = {'dates': [], 'counts': []}
                
        except Exception as e:
            st.warning(f"Erreur pr√©paration donn√©es: {e}")
            wordcloud_text = "Erreur pr√©paration wordcloud"
            chart_data = {'dates': [], 'counts': []}
        
        # Stocker les donn√©es
        st.session_state.video_data = {
            'url': url,
            'video_title': video_title,
            'sync_date': sync_date,
            'total_comments': total_comments,
            'most_liked_comment': most_liked_comment,
            'most_liked_date': most_liked_date,
            'like_count': like_count,
            'wordcloud_text': wordcloud_text,
            'comments_per_day': chart_data
        }
        
        st.session_state.analysis_done = True
        Load().data_base_deconnexion(client)
        return True
        
    except Exception as e:
        st.error(f"Erreur lors de l'analyse: {e}")
        Load().data_base_deconnexion(client)
        return False


# @st.cache_data
def main():
    # Initialiser les variables de session
    initialize_session_state()
    
    # Titre
    st.title("YOU REVIEW ")
    st.subheader("L'Analyse automatis√©e de vos commentaires YouTube", divider="gray")
    
    # Formulaire pour l'URL
    url = st.text_input("L'url de la vid√©o", value=st.session_state.url_input, key="url")
    st.session_state['url_input'] = url
    
    # Boutons
    col1, col2 = st.columns([1, 1])
    
    with col1:
        analyze_button = st.button("Lancer l'analyse")
    
    with col2:
        if st.session_state.analysis_done:
            clear_button = st.button("Effacer les r√©sultats")
            if clear_button:
                st.session_state.analysis_done = False
                st.session_state.video_data = {}
                st.session_state.url_input = ""
                st.rerun()
    
    # Si on clique sur "Lancer l'analyse"
    if analyze_button:
        if not url:
            st.error("Le champ de l'URL ne peut pas √™tre vide.")
            return
        
        if perform_analysis(url):
            st.rerun()  # Recharger la page pour afficher les r√©sultats
    
    # Afficher les r√©sultats s'ils existent
    if st.session_state.analysis_done:
        display_analysis_results()
    
    # Message d'aide si pas d'analyse
    #elif not st.session_state.analysis_done and not analyze_button:
        #st.info("üëÜ Entrez une URL YouTube et cliquez sur 'Lancer l'analyse' pour commencer.")


    # # Variable pour savoir si on doit faire l'extraction
    # data_exists = False
    # db = None
    # # titre 
    # st.title("YOU REVIEW ")
    # st.subheader("L'Analyse automatis√© de vos commentaires YouTube", divider="gray")
    # # formulaire pour l'url de la vid√©o et le pseudo du youtubeur
    # url = st.text_input("L'url de la vid√©o", key="url")
    # # si l'url est dans le session state, on la r√©cup√®re
    # st.session_state['url_input'] = url

    # if st.button("Lancer l'analyse"):
    #     if not url:
    #         st.error("le champ de l'URL ne peut pas √™tre vide.")   
    #         return 
    #     try:
    #         extract = Extraction(video_url=url)
    #         videoid = extract.url2id()
    #         st.session_state['videoid'] = videoid
    #     except Exception as e:
    #         st.error(f"Erreur veuillez entrer une URL You Tube valide. : {e}")
    #         raise Exception(f"Erreur veuillez entrer une URL You Tube valide. : {e}")
        
    #     client = Load().data_base_connexion()
    #     st.session_state['client_mdb'] = client

    #     try:
    #         db_name = get_existing_db(client, videoid)
    #         st.session_state['data_base_name'] = str(db_name)
    #     except Exception as e:
    #         st.error(f"Erreur lors de get_existing_db : {e}")
    #         raise Exception(f"Erreur lors de get_existing_db : {e}")

    #     try:
    #         db = client[db_name]
    #         if videoid in db.list_collection_names(): # and db[videoid].count_documents({}) > 0:
    #             st.success("La video existe d√©j√† dans la base de donn√©es")
    #             data_exists = True
    #     except Exception as e:
    #         data_exists = False
    #         st.error(f"Les donn√©es n'existent pas encore, on lance l'extration des donn√©es ...")
        
    #         if not data_exists:
    #             with st.spinner("Wait for it...", show_time=True):
    #                 chanel_id = main_etl(url, with_channel_id=True)
    #                 st.success("Done!")
    #         # st.success("Extraction termin√©e avec succ√®s !")
    #         try:
    #             db = client[chanel_id]

    #         except Exception as e:
    #             st.error(f"Erreur lors de l'extraction du channel_id : {e}")
    #             raise Exception(f"Erreur lors de l'extraction du channel_id : {e}")

    #     if db is not None:
            
    #         viedo_title, synchronisation_date, total_comments, most_liked_comment, most_liked_comment_publish_date, like_count = get_kpi(db, videoid)
    #         # afficher le titre de la vid√©o analys√©e
    #         st.header(f"Analyse de la vid√©o :\n ")
    #         st.subheader(f"{viedo_title}")
    #         # afficher la video :
    #         st.video(url)
    #         # afficher la date de mise √† jours des donn√©es 
    #         st.caption(f"Date de derni√®re mise √† jour : {synchronisation_date}")
    #         st.header("Analyse de premier niveau")

    #         # cr√©ation de colonnes
    #         left, right = st.columns([1,1], border=False)
            

    #         # afficher le nombre de commentaires
    #         left.metric(label="Nombre total de commentaires", value= db[videoid].count_documents({}), border=True)
    #         # afficher le commentaire avec le plus de likes
    #         # most_liked_comment = db[videoid].find_one(sort=[("likeCount", -1)])["comment"]
    #         most_liked_comment = most_liked_comment if most_liked_comment else "Aucun commentaire trouv√©"
    #         # st.metric(label="Commentaire le plus aim√©", value=str(most_liked_comment),border=True)

    #         # with left.container(border=True):
    #         left_container = left.container(border=True)
    #         left_container.header("Commentaire le plus aim√©")
    #         left_container.subheader(most_liked_comment)
    #         left_container.caption(f"Publi√© le : {most_liked_comment_publish_date}")
    #         left_container.caption(f"Nombre de likes : {like_count}")

            
    #         df = pd.DataFrame(list( db[videoid].find()))
    #         # text = df['tokens_clean_lem'].dropna().astype(str).str.cat(sep=" ")
    #         text = df['comment_clean_lem'].dropna()
    #         make_wordcloud(" ".join(text), right)

    #         # afficher le commentaire le plus long
    #         # afficher l'√©volution du nombre de commentaires au fil du temps
    #         # df = pd.DataFrame(list(db[videoid].find(
    #         #     {},                      # filtre : vide pour tous les documents
    #         #     {"publishedAt": 1, "comment": 1, "_id": 0}  # projection : 1 pour inclure, 0 pour exclure _id
    #         # )))
    #         # # le nombre de commentaires par jour
    #         # # Conversion en date simple
    #         # df['date'] = pd.to_datetime(df['publishedAt']).dt.date
    #         # # Cr√©ation de la figure
    #         # fig, ax = plt.subplots(figsize=(12, 6))
    #         # df['date'].value_counts().sort_index().plot(kind='bar', ax=ax)
    #         # # Titres et labels
    #         # ax.set_title('Nombre de commentaires par jour')
    #         # ax.set_xlabel('Date')
    #         # ax.set_ylabel('Nombre de commentaires')
    #         # plt.xticks(rotation=45)
    #         # plt.tight_layout()
    #         # # Affichage dans Streamlit
    #         # st.pyplot(fig)

    #         # Le nombre de commentaires par jour avec l'heure
    #         # Le nombre de commentaires par jour
    #         df = pd.DataFrame(list(db[videoid].find(
    #             {}, # filtre : vide pour tous les documents
    #             {"publishedAt": 1, "comment": 1, "_id": 0} # projection : 1 pour inclure, 0 pour exclure _id
    #         )))

    #         # Conversion en date simple
    #         df['date'] = pd.to_datetime(df['publishedAt']).dt.date

    #         # Compter les commentaires par jour
    #         comments_per_day = df['date'].value_counts().sort_index()

    #         # Cr√©ation de l'histogramme avec Plotly
    #         import plotly.express as px

    #         fig = px.bar(x=comments_per_day.index, 
    #                     y=comments_per_day.values,
    #                     title='Nombre de commentaires par jour',
    #                     labels={'x': 'Date', 'y': 'Nombre de commentaires'})

    #         # Affichage dans Streamlit
    #         st.plotly_chart(fig, use_container_width=True)

            

    #         Load().data_base_deconnexion(client)



        # mentions l√©gales
        # politque de confidentielit√©
        # condition g√©rale d'utilisation

        # doc de formation
        # doc technique